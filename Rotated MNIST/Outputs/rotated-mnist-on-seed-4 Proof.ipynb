{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport numpy as np\nimport random\n# import escnn\n# from escnn import gspaces\n# from escnn import nn as enn\n\nclass Z2CNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(Z2CNN, self).__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 20, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n        self.conv6 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n        self.conv7 = nn.Conv2d(20, 20, kernel_size=4)  # No padding for 4x4 kernel\n\n        # Batch normalization\n        self.bn1 = nn.BatchNorm2d(20)\n        self.bn2 = nn.BatchNorm2d(20)\n        self.bn3 = nn.BatchNorm2d(20)\n        self.bn4 = nn.BatchNorm2d(20)\n        self.bn5 = nn.BatchNorm2d(20)\n        self.bn6 = nn.BatchNorm2d(20)\n        self.bn7 = nn.BatchNorm2d(20)\n\n        # Pooling and dropout\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # After conv2\n        self.dropout = nn.Dropout(p=0.5)\n\n        # Fully connected layer\n        self.fc = nn.Linear(20, num_classes)  # Final layer output\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)  # Max-pool after conv2\n        x = self.dropout(x)\n\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.relu(self.bn6(self.conv6(x)))\n        x = F.relu(self.bn7(self.conv7(x)))\n\n        # Global average pooling\n        x = torch.mean(x, dim=(2, 3))  # Reduces spatial dims to 1x1\n        x = self.fc(x)\n        return x\n\n# Helper to rotate regular conv filters\ndef rotate_2d_tensor(tensor, k: int):\n    if k % 4 == 0:\n        return tensor\n    elif k % 4 == 1:\n        return torch.flip(tensor.transpose(-2, -1), dims=[-2])\n    elif k % 4 == 2:\n        return torch.flip(tensor, dims=[-2, -1])\n    elif k % 4 == 3:\n        return torch.flip(tensor.transpose(-2, -1), dims=[-1])\n\n# Rotate group-aware filters: shape (Cout, Cin, 4, Kh, Kw)\ndef rotate_g_filter(tensor, k: int):\n    Cout, Cin, G, Kh, Kw = tensor.shape\n    rotated = []\n\n    for g in range(G):\n        patch = tensor[:, :, g]  # shape (Cout, Cin, Kh, Kw)\n        rotated_patch = rotate_2d_tensor(patch, k)  # shape (Cout, Cin, Kh, Kw)\n        rotated.append(rotated_patch)\n\n    rotated = torch.stack(rotated, dim=2)  # (Cout, Cin, 4, Kh, Kw)\n    rotated = torch.roll(rotated, shifts=k, dims=2)  # roll group axis\n    return rotated\n\n# G-CNN block\nclass Partial_GConvZ2toP4(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(Partial_GConvZ2toP4, self).__init__()\n        self.G_size = 4 # For P4\n        self.base_filters = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))\n        # FIX 1: Initialize alpha_logits to encourage alphas near 1.0 initially\n        self.alpha_logits = nn.Parameter(torch.ones(self.G_size, dtype=torch.float32) * 2.0, requires_grad=True)\n        nn.init.kaiming_normal_(self.base_filters, nonlinearity='relu')\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x):\n        # x: (B, C_in, H, W)\n        outputs = []\n        alphas = torch.sigmoid(self.alpha_logits)\n\n        for r_idx in range(self.G_size): # Iterate 0 to 3 for P4\n            # Rotate base filter for the r-th group element\n            rotated_filter = torch.rot90(self.base_filters, k=r_idx, dims=(-2, -1))\n            out_conv = F.conv2d(x, rotated_filter, stride=self.stride, padding=self.padding)\n            \n            # Apply alpha weighting\n            out_weighted = out_conv * alphas[r_idx] # Broadcast alpha\n            outputs.append(out_weighted.unsqueeze(2))  # (B, out_c, 1, H, W)\n\n        return torch.cat(outputs, dim=2)  # (B, out_c, G_size, H, W)\n\nclass Partial_GConvP4toP4(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(Partial_GConvP4toP4, self).__init__()\n        self.G_size = 4 # For P4\n        # base_filters: (C_out, C_in, G_in_orientations, k, k)\n        self.base_filters = nn.Parameter(torch.empty(out_channels, in_channels, self.G_size, kernel_size, kernel_size))\n        # FIX 1: Initialize alpha_logits to encourage alphas near 1.0 initially\n        self.alpha_logits = nn.Parameter(torch.ones(self.G_size, dtype=torch.float32) * 2.0, requires_grad=True)\n        \n        # Initialize each of the G_size filter banks\n        for g_idx in range(self.G_size):\n            nn.init.kaiming_normal_(self.base_filters[:, :, g_idx, :, :], nonlinearity='relu')\n        \n        self.stride = stride\n        self.padding = padding\n        self.kernel_size_val = kernel_size # Store kernel_size as an int or tuple\n\n    def forward(self, x):\n        # x: (B, C_in, G_in_orientations, H, W)\n        B, C_in, G_in, H_in, W_in = x.shape\n        C_out = self.base_filters.shape[0]\n        \n        # Determine kernel_size for calculation (if it's a tuple or int)\n        k_h = self.kernel_size_val if isinstance(self.kernel_size_val, int) else self.kernel_size_val[0]\n        k_w = self.kernel_size_val if isinstance(self.kernel_size_val, int) else self.kernel_size_val[1]\n\n        # Calculate output spatial dimensions CORRECTLY\n        out_H_actual = (H_in + 2 * self.padding - (k_h - 1) - 1) // self.stride + 1\n        out_W_actual = (W_in + 2 * self.padding - (k_w - 1) - 1) // self.stride + 1\n        \n        outputs = []\n        alphas = torch.sigmoid(self.alpha_logits)\n\n        # Loop over output orientations (g_out_idx from 0 to G_size-1)\n        for g_out_idx in range(self.G_size):\n            # Initialize out_sum_for_g_out with CORRECT output dimensions\n            out_sum_for_g_out = torch.zeros(B, C_out, out_H_actual, out_W_actual, device=x.device, dtype=x.dtype)\n            \n            # Loop over input orientations (g_in_idx from 0 to G_in-1)\n            for g_in_idx in range(G_in):\n                # Get input slice for this input orientation\n                inp_slice = x[:, :, g_in_idx, :, :]  # (B, C_in, H_in, W_in)\n                \n                # Get the base filter for this input orientation\n                filter_for_g_in = self.base_filters[:, :, g_in_idx, :, :] # (C_out, C_in, k, k)\n                \n                # Rotate this filter according to the output orientation g_out_idx\n                # This is L_{g_out} (\\psi_{g_in})\n                transformed_filter = torch.rot90(filter_for_g_in, k=g_out_idx, dims=(-2, -1))\n                \n                conv_result = F.conv2d(inp_slice, transformed_filter, stride=self.stride, padding=self.padding)\n                # Now conv_result (B, C_out, out_H_actual, out_W_actual) can be added to out_sum_for_g_out\n                out_sum_for_g_out += conv_result\n            \n            # Apply alpha weighting for this output orientation\n            weighted_out_sum = out_sum_for_g_out * alphas[g_out_idx]\n            outputs.append(weighted_out_sum.unsqueeze(2))  # (B, C_out, 1, H_out, W_out)\n\n        return torch.cat(outputs, dim=2)  # (B, C_out, G_size, H_out, W_out)\n\n\nclass GroupMaxPool(nn.Module):\n    def forward(self, x):\n        # x: (B, C, G, H, W)\n        return torch.max(x, dim=2)[0]  # â†’ (B, C, H, W)\n\n\nclass Partial_GCNN(nn.Module):\n    def __init__(self, num_classes=10, input_channels=1): # MNIST has 1 input channel\n        super(Partial_GCNN, self).__init__()\n        self.G_size = 4 # For P4\n\n        c = 10 # Number of output channels for G-Conv layers (as in user's original code)\n\n        self.gconv1 = Partial_GConvZ2toP4(input_channels, c, kernel_size=3, padding=1)\n        # FIX 2: Use BatchNorm2d\n        self.bn1 = nn.BatchNorm2d(c * self.G_size)\n\n        self.gconv2 = Partial_GConvP4toP4(c, c, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(c * self.G_size)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Spatial pooling\n\n        self.gconv3 = Partial_GConvP4toP4(c, c, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(c * self.G_size)\n\n        self.gconv4 = Partial_GConvP4toP4(c, c, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(c * self.G_size)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # Spatial pooling\n\n        self.gconv5 = Partial_GConvP4toP4(c, c, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(c * self.G_size)\n\n        self.gconv6 = Partial_GConvP4toP4(c, c, kernel_size=3, padding=1)\n        self.bn6 = nn.BatchNorm2d(c * self.G_size)\n\n        self.gconv7 = Partial_GConvP4toP4(c, c, kernel_size=4, stride=1, padding=0)\n        self.bn7 = nn.BatchNorm2d(c * self.G_size)\n\n        self.group_pool = GroupMaxPool()\n        # The output of gconv7 (after bn7 and reshape) will be (B, c, G, H', W')\n        # After group_pool: (B, c, H', W')\n        # For MNIST, H'=4, W'=4 after conv7 (28->14->7->4)\n        # So, input to fc is c * 4 * 4\n        self.fc = nn.Linear(c * 4 * 4, num_classes)\n\n\n    def forward(self, x):\n        # Block 1\n        x = self.gconv1(x) # -> (B, c, G, H, W)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out) # Reshape for BN\n        x = F.relu(self.bn1(x))\n        x = x.view(B, C_out, G, H_out, W_out) # Reshape back\n\n        # Block 2\n        x = self.gconv2(x) # -> (B, c, G, H, W)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out) # Reshape for BN\n        x = F.relu(self.bn2(x))\n        # Spatial pool operates on 4D tensor (B, C*G, H, W)\n        x = self.pool1(x) # -> (B, c*G, H/2, W/2)\n        # Reshape back for next G-Conv\n        B, _, H_pooled, W_pooled = x.shape # C*G is already there\n        x = x.view(B, C_out, G, H_pooled, W_pooled)\n\n        # Block 3\n        x = self.gconv3(x) # -> (B, c, G, H_curr, W_curr)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out)\n        x = F.relu(self.bn3(x))\n        x = x.view(B, C_out, G, H_out, W_out)\n\n        # Block 4\n        x = self.gconv4(x) # -> (B, c, G, H_curr, W_curr)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out)\n        x = F.relu(self.bn4(x))\n        # Spatial pool\n        x = self.pool2(x) # -> (B, c*G, H_curr/2, W_curr/2)\n        B, _, H_pooled, W_pooled = x.shape\n        x = x.view(B, C_out, G, H_pooled, W_pooled)\n\n        # Block 5\n        x = self.gconv5(x) # -> (B, c, G, H_curr, W_curr)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out)\n        x = F.relu(self.bn5(x))\n        x = x.view(B, C_out, G, H_out, W_out)\n\n        # Block 6\n        x = self.gconv6(x) # -> (B, c, G, H_curr, W_curr)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out)\n        x = F.relu(self.bn6(x))\n        x = x.view(B, C_out, G, H_out, W_out)\n\n        # Block 7\n        x = self.gconv7(x) # -> (B, c, G, H_final, W_final)\n        B, C_out, G, H_out, W_out = x.shape\n        x = x.view(B, C_out * G, H_out, W_out)\n        x = F.relu(self.bn7(x))\n        x = x.view(B, C_out, G, H_out, W_out) # H_final, W_final should be 4x4 for MNIST\n\n        # Pooling and FC\n        x = self.group_pool(x) # -> (B, c, H_final, W_final)\n        x = x.view(x.size(0), -1) # Flatten: (B, c * H_final * W_final)\n        x = self.fc(x)\n        return x\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef expected_calibration_error(conf, correct, n_bins=15):\n    bins = torch.linspace(0, 1, n_bins + 1)\n    ece  = torch.zeros(1, device=conf.device)\n    for i in range(n_bins):\n        mask   = (conf > bins[i]) & (conf <= bins[i + 1])\n        if mask.any():\n            acc   = correct[mask].float().mean()\n            bin_conf = conf[mask].mean()\n            ece  += mask.float().mean() * (acc - bin_conf).abs()\n    return ece.item()\n\n@torch.no_grad()\ndef compute_ece(model, test_loader, device=\"cuda\", n_bins=15):\n    model.eval()\n    all_conf, all_correct = [], []\n\n    for x, y in test_loader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)                       # (batch, C)\n        prob   = F.softmax(logits, dim=1)       # convert to probabilities\n        conf, pred = prob.max(dim=1)            # highest prob per sample\n        all_conf.append(conf)\n        all_correct.append(pred.eq(y))          # Boolean tensor\n\n    conf_tensor    = torch.cat(all_conf)        # shape (N,)\n    correct_tensor = torch.cat(all_correct)     # shape (N,)\n\n    return expected_calibration_error(conf_tensor,\n                                      correct_tensor,\n                                      n_bins=n_bins)\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, ConcatDataset, random_split\n\n\ndef get_rotated_mnist(batch_size: int = 128, seed: int = 42):\n\n    # ---------------- transforms -----------------------------------\n    train_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),   # FMNIST mean / std\n        transforms.RandomRotation(degrees=(-180, 180)),   \n    ])\n\n    test_transform_no_rotation = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),   # FMNIST mean / std        \n    ])\n\n    test_transform_with_rotation = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),   # FMNIST mean / std\n        transforms.RandomRotation(degrees=(-180, 180)),   \n    ])\n\n    # OptionÂ C in code (example)\n    full_train = datasets.MNIST(root='./data', train=True,\n                                       download=True, transform=train_transform)\n\n    train_ds, val_ds = random_split(\n        full_train, [55_000, 5_000],\n        generator=torch.Generator().manual_seed(seed)\n    )\n\n    test_ds_no_rotation = datasets.MNIST(root='./data', train=False,\n                                    download=True, transform=test_transform_no_rotation)\n\n    test_ds_with_rotation = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform_with_rotation)\n\n    \n\n    # ---------------- loaders --------------------------------------\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n    test_loader_no_rotation  = DataLoader(test_ds_no_rotation,  batch_size=batch_size, shuffle=False)\n    test_loader_with_rotation = DataLoader(test_ds_with_rotation, batch_size=batch_size, shuffle=False)    \n\n    return train_loader, val_loader, test_loader_no_rotation, test_loader_with_rotation\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    train_loss = 0\n    correct = 0\n    criterion = nn.CrossEntropyLoss()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\n    train_loss /= len(train_loader.dataset)\n    accuracy = 100. * correct / len(train_loader.dataset)\n    print(f'Train Epoch: {epoch} | Loss: {train_loss:.4f} | Accuracy: {accuracy:.2f}%')\n\ndef test(model, device, test_loader, test_type):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    criterion = nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print(f'{test_type}: Average loss: {test_loss:.4f} | Accuracy: {accuracy:.2f}%\\n')\n    return accuracy\n\n# Training loop\ndef training_loop(model, device, train_loader, test_loader_no_rotation, test_loader_with_rotation, val_loader, hyper_parameters):\n    # --- Training Loop ---\n\n    MODEL_NAME = type(model).__name__\n    print(f\"Training {type(model).__name__} Model...\")\n    print(f\"Total parameters: {count_parameters(model):,}\")\n\n    LEARNING_RATE = hyper_parameters[\"LEARNING_RATE\"]\n    WEIGHT_DECAY = hyper_parameters[\"WEIGHT_DECAY\"]\n    MOMENTUM = hyper_parameters[\"MOMENTUM\"]\n    EPOCHS = hyper_parameters[\"EPOCHS\"]\n    SCHEDULER_PATIENCE = hyper_parameters[\"SCHEDULER_PATIENCE\"]\n    SCHEDULER_FACTOR = hyper_parameters[\"SCHEDULER_FACTOR\"]\n    EARLY_STOPPING_PATIENCE = hyper_parameters[\"EARLY_STOPPING_PATIENCE\"]\n    BEST_MODEL_PATH = \"/kaggle/working/\" + MODEL_NAME + \"_best_tuned.pth\"\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120], gamma=0.1)\n\n    print(\"\\nStarting Training...\")\n    best_val_accuracy = 0.0\n    epochs_no_improve = 0 # Counter for early stopping\n\n    for epoch in range(1, EPOCHS + 1):\n        print(f\"\\n--- Epoch {epoch}/{EPOCHS} ---\")\n        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n        train(model, device, train_loader, optimizer, epoch)\n        val_accuracy = test(model, device, val_loader, \"Validation set\") # Get validation accuracy\n\n        # if MODEL_NAME == \"Partial_GCNN\":\n        #     print(\"Alphas:\", [getattr(model, f\"gconv{i}\").alpha_logits.detach().cpu().numpy().round(3).tolist() for i in range(1, 8)])\n\n        # Step the scheduler based on validation accuracy\n        scheduler.step()\n\n        # Check for improvement and save best model\n        if val_accuracy > best_val_accuracy:\n            print(f\"Validation accuracy improved ({best_val_accuracy:.2f}% -> {val_accuracy:.2f}%). Saving model...\")\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), BEST_MODEL_PATH)\n            epochs_no_improve = 0 # Reset counter\n        else:\n            epochs_no_improve += 1\n            print(f\"Validation accuracy did not improve for {epochs_no_improve} epoch(s).\")\n\n        # Early stopping check\n        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n            print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n            break\n\n    print(\"\\nTraining finished.\")\n\n    # --- Final Evaluation on Test Set ---\n    print(f\"\\nLoading best model from {BEST_MODEL_PATH} for final evaluation...\")\n    try:\n        model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n        test_accuracy_no_rotation = test(model, device, test_loader_no_rotation, \"Test set no rotation\")\n        test_accuracy_with_rotation = test(model,device, test_loader_with_rotation, \"Test set with rotation\")\n        print(f\"\\nFinal Test Accuracy without test augmentation (Best {type(model).__name__} Model): {test_accuracy_no_rotation:.2f}%\")\n        print(f\"\\nFinal Test Accuracy with test augmentation (Best {type(model).__name__} Model): {test_accuracy_with_rotation:.2f}%\")\n\n    except FileNotFoundError:\n        print(\"Best model file not found. Run training first.\")\n    except Exception as e:\n         print(f\"An error occurred loading the best model: {e}\")\n\n    return test_accuracy_no_rotation, test_accuracy_with_rotation\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load data\ntrain_loader, val_loader, test_loader_no_rotation, test_loader_with_rotation = get_rotated_mnist()\n\n# Hyperparameters\nhyper_parameters = {\n  \"LEARNING_RATE\": 0.1,\n  \"WEIGHT_DECAY\": 5e-4,\n  \"MOMENTUM\": 0.9,\n  \"EPOCHS\": 150,\n  \"SCHEDULER_PATIENCE\": 10,\n  \"SCHEDULER_FACTOR\": 0.5,\n  \"EARLY_STOPPING_PATIENCE\": 25,\n}\n\nseeds   = [4]\n\nmodels  = {#\"Z2\": Z2CNN,\n           #\"P4\": GCNN,\n           \"P4-W\": Partial_GCNN}\n\n# --------------- results dict ------------------------------------- #\nresults = {name: {\"no_rotation\": [], \"rotation\": []} for name in models}\nece_results = {name: {\"no_rotation\": [], \"rotation\": []} for name in models}\n\n# ------------------------------------------------------------------ #\nfor seed in seeds:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark     = False\n\n    for name, ModelClass in models.items():\n        model = ModelClass().to(device)\n        best_acc_no_rotation, best_acc_with_rotation = training_loop(model, device, train_loader, test_loader_no_rotation, test_loader_with_rotation, val_loader, hyper_parameters)\n\n        results[name][\"no_rotation\"].append(best_acc_no_rotation)\n        print(f\"seed {seed} \\t{name} (No Rotation)\\t{best_acc_no_rotation:.2f}%\")\n\n        results[name][\"rotation\"].append(best_acc_with_rotation)\n        print(f\"seed {seed} \\t{name} (With Rotation)\\t{best_acc_with_rotation:.2f}%\")\n\n        ece_no_rotation = compute_ece(model, test_loader_no_rotation, device=\"cuda\", n_bins=15)\n        ece_results[name][\"no_rotation\"].append(ece_no_rotation)\n\n        print(f\"Expected Calibration Error no test rotation: {ece_no_rotation:.4f}\")\n\n        ece_with_rotation = compute_ece(model, test_loader_with_rotation, device=\"cuda\", n_bins=15)\n        ece_results[name][\"rotation\"].append(ece_with_rotation)\n\n        print(f\"Expected Calibration Error with test rotation: {ece_with_rotation:.4f}\")\n\n\n# ----------------- summary ------------------ #\nfor name, score_dict in results.items():\n    mean_no_rotation = np.mean(score_dict[\"no_rotation\"])\n    std_no_rotation  = np.std(score_dict[\"no_rotation\"])\n    mean_with_rotation = np.mean(score_dict[\"rotation\"])\n    std_with_rotation  = np.std(score_dict[\"rotation\"])\n    \n    print(f\"{name:8s}: No Rotation - {mean_no_rotation:.2f} Â± {std_no_rotation:.2f} (n={len(score_dict['no_rotation'])})\")\n    print(f\"{name:8s}: With Rotation - {mean_with_rotation:.2f} Â± {std_with_rotation:.2f} (n={len(score_dict['rotation'])})\")\n\n\nprint(\"\\n------- ECE Results ------------\")\nfor name, ece_dict in ece_results.items():\n    mean_no_rotation = np.mean(ece_dict[\"no_rotation\"])\n    mean_with_rotation = np.mean(ece_dict[\"rotation\"])\n    print(f\"{name:8s}: No Rotation - {mean_no_rotation}\")\n    print(f\"{name:8s}: With Rotation - {mean_with_rotation}\")","metadata":{"_uuid":"3b2ac1bf-7763-4b3f-93b3-8ae92302fcbd","_cell_guid":"64f34ca5-2582-4198-bf53-a88acff73d58","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T12:46:26.229527Z","iopub.execute_input":"2025-05-18T12:46:26.230036Z","iopub.status.idle":"2025-05-18T14:48:54.453612Z","shell.execute_reply.started":"2025-05-18T12:46:26.230011Z","shell.execute_reply":"2025-05-18T14:48:54.452803Z"}},"outputs":[{"name":"stdout","text":"Training Partial_GCNN Model...\nTotal parameters: 26,688\n\nStarting Training...\n\n--- Epoch 1/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 1 | Loss: 0.0063 | Accuracy: 73.73%\nValidation set: Average loss: 0.0023 | Accuracy: 91.06%\n\nValidation accuracy improved (0.00% -> 91.06%). Saving model...\n\n--- Epoch 2/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 2 | Loss: 0.0019 | Accuracy: 92.24%\nValidation set: Average loss: 0.0028 | Accuracy: 89.50%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 3/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 3 | Loss: 0.0016 | Accuracy: 93.50%\nValidation set: Average loss: 0.0019 | Accuracy: 92.30%\n\nValidation accuracy improved (91.06% -> 92.30%). Saving model...\n\n--- Epoch 4/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 4 | Loss: 0.0015 | Accuracy: 94.17%\nValidation set: Average loss: 0.0018 | Accuracy: 93.34%\n\nValidation accuracy improved (92.30% -> 93.34%). Saving model...\n\n--- Epoch 5/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 5 | Loss: 0.0014 | Accuracy: 94.44%\nValidation set: Average loss: 0.0019 | Accuracy: 92.78%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 6/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 6 | Loss: 0.0014 | Accuracy: 94.48%\nValidation set: Average loss: 0.0019 | Accuracy: 92.56%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 7/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 7 | Loss: 0.0013 | Accuracy: 94.72%\nValidation set: Average loss: 0.0019 | Accuracy: 93.32%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 8/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 8 | Loss: 0.0013 | Accuracy: 94.84%\nValidation set: Average loss: 0.0017 | Accuracy: 93.40%\n\nValidation accuracy improved (93.34% -> 93.40%). Saving model...\n\n--- Epoch 9/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 9 | Loss: 0.0012 | Accuracy: 95.07%\nValidation set: Average loss: 0.0018 | Accuracy: 93.32%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 10/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 10 | Loss: 0.0012 | Accuracy: 95.18%\nValidation set: Average loss: 0.0016 | Accuracy: 93.70%\n\nValidation accuracy improved (93.40% -> 93.70%). Saving model...\n\n--- Epoch 11/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 11 | Loss: 0.0012 | Accuracy: 95.07%\nValidation set: Average loss: 0.0015 | Accuracy: 94.50%\n\nValidation accuracy improved (93.70% -> 94.50%). Saving model...\n\n--- Epoch 12/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 12 | Loss: 0.0012 | Accuracy: 95.13%\nValidation set: Average loss: 0.0019 | Accuracy: 92.72%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 13/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 13 | Loss: 0.0012 | Accuracy: 95.31%\nValidation set: Average loss: 0.0014 | Accuracy: 94.80%\n\nValidation accuracy improved (94.50% -> 94.80%). Saving model...\n\n--- Epoch 14/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 14 | Loss: 0.0012 | Accuracy: 95.34%\nValidation set: Average loss: 0.0016 | Accuracy: 94.12%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 15/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 15 | Loss: 0.0011 | Accuracy: 95.48%\nValidation set: Average loss: 0.0016 | Accuracy: 93.84%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 16/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 16 | Loss: 0.0011 | Accuracy: 95.54%\nValidation set: Average loss: 0.0013 | Accuracy: 95.32%\n\nValidation accuracy improved (94.80% -> 95.32%). Saving model...\n\n--- Epoch 17/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 17 | Loss: 0.0011 | Accuracy: 95.49%\nValidation set: Average loss: 0.0016 | Accuracy: 93.98%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 18/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 18 | Loss: 0.0011 | Accuracy: 95.45%\nValidation set: Average loss: 0.0013 | Accuracy: 94.74%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 19/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 19 | Loss: 0.0011 | Accuracy: 95.57%\nValidation set: Average loss: 0.0012 | Accuracy: 95.32%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 20/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 20 | Loss: 0.0011 | Accuracy: 95.56%\nValidation set: Average loss: 0.0015 | Accuracy: 94.78%\n\nValidation accuracy did not improve for 4 epoch(s).\n\n--- Epoch 21/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 21 | Loss: 0.0011 | Accuracy: 95.72%\nValidation set: Average loss: 0.0018 | Accuracy: 93.42%\n\nValidation accuracy did not improve for 5 epoch(s).\n\n--- Epoch 22/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 22 | Loss: 0.0011 | Accuracy: 95.65%\nValidation set: Average loss: 0.0013 | Accuracy: 94.68%\n\nValidation accuracy did not improve for 6 epoch(s).\n\n--- Epoch 23/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 23 | Loss: 0.0011 | Accuracy: 95.72%\nValidation set: Average loss: 0.0018 | Accuracy: 93.56%\n\nValidation accuracy did not improve for 7 epoch(s).\n\n--- Epoch 24/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 24 | Loss: 0.0011 | Accuracy: 95.62%\nValidation set: Average loss: 0.0017 | Accuracy: 93.28%\n\nValidation accuracy did not improve for 8 epoch(s).\n\n--- Epoch 25/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 25 | Loss: 0.0011 | Accuracy: 95.75%\nValidation set: Average loss: 0.0016 | Accuracy: 93.76%\n\nValidation accuracy did not improve for 9 epoch(s).\n\n--- Epoch 26/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 26 | Loss: 0.0010 | Accuracy: 95.81%\nValidation set: Average loss: 0.0015 | Accuracy: 94.88%\n\nValidation accuracy did not improve for 10 epoch(s).\n\n--- Epoch 27/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 27 | Loss: 0.0011 | Accuracy: 95.83%\nValidation set: Average loss: 0.0013 | Accuracy: 95.26%\n\nValidation accuracy did not improve for 11 epoch(s).\n\n--- Epoch 28/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 28 | Loss: 0.0010 | Accuracy: 95.81%\nValidation set: Average loss: 0.0017 | Accuracy: 93.42%\n\nValidation accuracy did not improve for 12 epoch(s).\n\n--- Epoch 29/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 29 | Loss: 0.0011 | Accuracy: 95.78%\nValidation set: Average loss: 0.0014 | Accuracy: 95.44%\n\nValidation accuracy improved (95.32% -> 95.44%). Saving model...\n\n--- Epoch 30/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 30 | Loss: 0.0011 | Accuracy: 95.79%\nValidation set: Average loss: 0.0015 | Accuracy: 94.52%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 31/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 31 | Loss: 0.0011 | Accuracy: 95.67%\nValidation set: Average loss: 0.0015 | Accuracy: 94.22%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 32/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 32 | Loss: 0.0010 | Accuracy: 95.82%\nValidation set: Average loss: 0.0015 | Accuracy: 95.02%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 33/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 33 | Loss: 0.0010 | Accuracy: 95.88%\nValidation set: Average loss: 0.0013 | Accuracy: 95.06%\n\nValidation accuracy did not improve for 4 epoch(s).\n\n--- Epoch 34/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 34 | Loss: 0.0011 | Accuracy: 95.75%\nValidation set: Average loss: 0.0012 | Accuracy: 95.42%\n\nValidation accuracy did not improve for 5 epoch(s).\n\n--- Epoch 35/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 35 | Loss: 0.0010 | Accuracy: 95.95%\nValidation set: Average loss: 0.0013 | Accuracy: 95.12%\n\nValidation accuracy did not improve for 6 epoch(s).\n\n--- Epoch 36/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 36 | Loss: 0.0010 | Accuracy: 95.85%\nValidation set: Average loss: 0.0014 | Accuracy: 95.04%\n\nValidation accuracy did not improve for 7 epoch(s).\n\n--- Epoch 37/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 37 | Loss: 0.0010 | Accuracy: 95.96%\nValidation set: Average loss: 0.0013 | Accuracy: 95.02%\n\nValidation accuracy did not improve for 8 epoch(s).\n\n--- Epoch 38/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 38 | Loss: 0.0010 | Accuracy: 95.86%\nValidation set: Average loss: 0.0014 | Accuracy: 94.36%\n\nValidation accuracy did not improve for 9 epoch(s).\n\n--- Epoch 39/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 39 | Loss: 0.0010 | Accuracy: 96.01%\nValidation set: Average loss: 0.0017 | Accuracy: 93.50%\n\nValidation accuracy did not improve for 10 epoch(s).\n\n--- Epoch 40/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 40 | Loss: 0.0010 | Accuracy: 96.03%\nValidation set: Average loss: 0.0014 | Accuracy: 95.00%\n\nValidation accuracy did not improve for 11 epoch(s).\n\n--- Epoch 41/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 41 | Loss: 0.0010 | Accuracy: 95.92%\nValidation set: Average loss: 0.0015 | Accuracy: 94.84%\n\nValidation accuracy did not improve for 12 epoch(s).\n\n--- Epoch 42/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 42 | Loss: 0.0010 | Accuracy: 95.90%\nValidation set: Average loss: 0.0011 | Accuracy: 96.02%\n\nValidation accuracy improved (95.44% -> 96.02%). Saving model...\n\n--- Epoch 43/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 43 | Loss: 0.0010 | Accuracy: 96.03%\nValidation set: Average loss: 0.0013 | Accuracy: 95.28%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 44/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 44 | Loss: 0.0010 | Accuracy: 96.03%\nValidation set: Average loss: 0.0012 | Accuracy: 95.30%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 45/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 45 | Loss: 0.0010 | Accuracy: 96.01%\nValidation set: Average loss: 0.0016 | Accuracy: 94.64%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 46/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 46 | Loss: 0.0010 | Accuracy: 95.99%\nValidation set: Average loss: 0.0013 | Accuracy: 95.14%\n\nValidation accuracy did not improve for 4 epoch(s).\n\n--- Epoch 47/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 47 | Loss: 0.0010 | Accuracy: 96.02%\nValidation set: Average loss: 0.0013 | Accuracy: 95.00%\n\nValidation accuracy did not improve for 5 epoch(s).\n\n--- Epoch 48/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 48 | Loss: 0.0010 | Accuracy: 96.02%\nValidation set: Average loss: 0.0014 | Accuracy: 94.74%\n\nValidation accuracy did not improve for 6 epoch(s).\n\n--- Epoch 49/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 49 | Loss: 0.0010 | Accuracy: 96.15%\nValidation set: Average loss: 0.0012 | Accuracy: 95.36%\n\nValidation accuracy did not improve for 7 epoch(s).\n\n--- Epoch 50/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 50 | Loss: 0.0010 | Accuracy: 95.96%\nValidation set: Average loss: 0.0014 | Accuracy: 94.70%\n\nValidation accuracy did not improve for 8 epoch(s).\n\n--- Epoch 51/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 51 | Loss: 0.0010 | Accuracy: 95.92%\nValidation set: Average loss: 0.0012 | Accuracy: 95.22%\n\nValidation accuracy did not improve for 9 epoch(s).\n\n--- Epoch 52/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 52 | Loss: 0.0010 | Accuracy: 96.08%\nValidation set: Average loss: 0.0021 | Accuracy: 91.26%\n\nValidation accuracy did not improve for 10 epoch(s).\n\n--- Epoch 53/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 53 | Loss: 0.0010 | Accuracy: 96.12%\nValidation set: Average loss: 0.0014 | Accuracy: 94.74%\n\nValidation accuracy did not improve for 11 epoch(s).\n\n--- Epoch 54/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 54 | Loss: 0.0010 | Accuracy: 96.05%\nValidation set: Average loss: 0.0010 | Accuracy: 95.94%\n\nValidation accuracy did not improve for 12 epoch(s).\n\n--- Epoch 55/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 55 | Loss: 0.0010 | Accuracy: 96.00%\nValidation set: Average loss: 0.0017 | Accuracy: 92.76%\n\nValidation accuracy did not improve for 13 epoch(s).\n\n--- Epoch 56/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 56 | Loss: 0.0010 | Accuracy: 96.17%\nValidation set: Average loss: 0.0012 | Accuracy: 95.58%\n\nValidation accuracy did not improve for 14 epoch(s).\n\n--- Epoch 57/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 57 | Loss: 0.0010 | Accuracy: 95.97%\nValidation set: Average loss: 0.0012 | Accuracy: 95.00%\n\nValidation accuracy did not improve for 15 epoch(s).\n\n--- Epoch 58/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 58 | Loss: 0.0010 | Accuracy: 96.19%\nValidation set: Average loss: 0.0010 | Accuracy: 96.08%\n\nValidation accuracy improved (96.02% -> 96.08%). Saving model...\n\n--- Epoch 59/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 59 | Loss: 0.0010 | Accuracy: 96.19%\nValidation set: Average loss: 0.0017 | Accuracy: 93.44%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 60/150 ---\nCurrent LR: 0.100000\nTrain Epoch: 60 | Loss: 0.0010 | Accuracy: 96.09%\nValidation set: Average loss: 0.0011 | Accuracy: 95.68%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 61/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 61 | Loss: 0.0006 | Accuracy: 97.56%\nValidation set: Average loss: 0.0006 | Accuracy: 97.82%\n\nValidation accuracy improved (96.08% -> 97.82%). Saving model...\n\n--- Epoch 62/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 62 | Loss: 0.0005 | Accuracy: 97.90%\nValidation set: Average loss: 0.0006 | Accuracy: 97.66%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 63/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 63 | Loss: 0.0005 | Accuracy: 97.98%\nValidation set: Average loss: 0.0006 | Accuracy: 97.74%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 64/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 64 | Loss: 0.0005 | Accuracy: 98.03%\nValidation set: Average loss: 0.0006 | Accuracy: 97.68%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 65/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 65 | Loss: 0.0005 | Accuracy: 98.14%\nValidation set: Average loss: 0.0006 | Accuracy: 97.72%\n\nValidation accuracy did not improve for 4 epoch(s).\n\n--- Epoch 66/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 66 | Loss: 0.0005 | Accuracy: 98.14%\nValidation set: Average loss: 0.0006 | Accuracy: 98.04%\n\nValidation accuracy improved (97.82% -> 98.04%). Saving model...\n\n--- Epoch 67/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 67 | Loss: 0.0005 | Accuracy: 98.13%\nValidation set: Average loss: 0.0005 | Accuracy: 97.96%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 68/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 68 | Loss: 0.0005 | Accuracy: 98.16%\nValidation set: Average loss: 0.0005 | Accuracy: 97.98%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 69/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 69 | Loss: 0.0004 | Accuracy: 98.25%\nValidation set: Average loss: 0.0005 | Accuracy: 98.08%\n\nValidation accuracy improved (98.04% -> 98.08%). Saving model...\n\n--- Epoch 70/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 70 | Loss: 0.0005 | Accuracy: 98.17%\nValidation set: Average loss: 0.0005 | Accuracy: 97.76%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 71/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 71 | Loss: 0.0004 | Accuracy: 98.26%\nValidation set: Average loss: 0.0006 | Accuracy: 98.16%\n\nValidation accuracy improved (98.08% -> 98.16%). Saving model...\n\n--- Epoch 72/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 72 | Loss: 0.0004 | Accuracy: 98.15%\nValidation set: Average loss: 0.0005 | Accuracy: 98.00%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 73/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 73 | Loss: 0.0004 | Accuracy: 98.23%\nValidation set: Average loss: 0.0006 | Accuracy: 97.66%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 74/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 74 | Loss: 0.0004 | Accuracy: 98.21%\nValidation set: Average loss: 0.0005 | Accuracy: 98.18%\n\nValidation accuracy improved (98.16% -> 98.18%). Saving model...\n\n--- Epoch 75/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 75 | Loss: 0.0005 | Accuracy: 98.27%\nValidation set: Average loss: 0.0006 | Accuracy: 97.72%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 76/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 76 | Loss: 0.0004 | Accuracy: 98.21%\nValidation set: Average loss: 0.0005 | Accuracy: 98.16%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 77/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 77 | Loss: 0.0004 | Accuracy: 98.21%\nValidation set: Average loss: 0.0005 | Accuracy: 97.88%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 78/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 78 | Loss: 0.0004 | Accuracy: 98.23%\nValidation set: Average loss: 0.0005 | Accuracy: 98.20%\n\nValidation accuracy improved (98.18% -> 98.20%). Saving model...\n\n--- Epoch 79/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 79 | Loss: 0.0004 | Accuracy: 98.32%\nValidation set: Average loss: 0.0006 | Accuracy: 97.74%\n\nValidation accuracy did not improve for 1 epoch(s).\n\n--- Epoch 80/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 80 | Loss: 0.0004 | Accuracy: 98.24%\nValidation set: Average loss: 0.0006 | Accuracy: 97.94%\n\nValidation accuracy did not improve for 2 epoch(s).\n\n--- Epoch 81/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 81 | Loss: 0.0004 | Accuracy: 98.28%\nValidation set: Average loss: 0.0005 | Accuracy: 97.88%\n\nValidation accuracy did not improve for 3 epoch(s).\n\n--- Epoch 82/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 82 | Loss: 0.0004 | Accuracy: 98.22%\nValidation set: Average loss: 0.0005 | Accuracy: 97.78%\n\nValidation accuracy did not improve for 4 epoch(s).\n\n--- Epoch 83/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 83 | Loss: 0.0004 | Accuracy: 98.22%\nValidation set: Average loss: 0.0005 | Accuracy: 98.00%\n\nValidation accuracy did not improve for 5 epoch(s).\n\n--- Epoch 84/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 84 | Loss: 0.0004 | Accuracy: 98.23%\nValidation set: Average loss: 0.0006 | Accuracy: 97.82%\n\nValidation accuracy did not improve for 6 epoch(s).\n\n--- Epoch 85/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 85 | Loss: 0.0004 | Accuracy: 98.25%\nValidation set: Average loss: 0.0005 | Accuracy: 98.04%\n\nValidation accuracy did not improve for 7 epoch(s).\n\n--- Epoch 86/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 86 | Loss: 0.0004 | Accuracy: 98.33%\nValidation set: Average loss: 0.0005 | Accuracy: 98.04%\n\nValidation accuracy did not improve for 8 epoch(s).\n\n--- Epoch 87/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 87 | Loss: 0.0004 | Accuracy: 98.28%\nValidation set: Average loss: 0.0006 | Accuracy: 97.48%\n\nValidation accuracy did not improve for 9 epoch(s).\n\n--- Epoch 88/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 88 | Loss: 0.0004 | Accuracy: 98.21%\nValidation set: Average loss: 0.0006 | Accuracy: 97.98%\n\nValidation accuracy did not improve for 10 epoch(s).\n\n--- Epoch 89/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 89 | Loss: 0.0004 | Accuracy: 98.29%\nValidation set: Average loss: 0.0005 | Accuracy: 98.02%\n\nValidation accuracy did not improve for 11 epoch(s).\n\n--- Epoch 90/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 90 | Loss: 0.0005 | Accuracy: 98.14%\nValidation set: Average loss: 0.0006 | Accuracy: 97.60%\n\nValidation accuracy did not improve for 12 epoch(s).\n\n--- Epoch 91/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 91 | Loss: 0.0005 | Accuracy: 98.10%\nValidation set: Average loss: 0.0006 | Accuracy: 98.10%\n\nValidation accuracy did not improve for 13 epoch(s).\n\n--- Epoch 92/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 92 | Loss: 0.0004 | Accuracy: 98.20%\nValidation set: Average loss: 0.0006 | Accuracy: 97.74%\n\nValidation accuracy did not improve for 14 epoch(s).\n\n--- Epoch 93/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 93 | Loss: 0.0004 | Accuracy: 98.26%\nValidation set: Average loss: 0.0006 | Accuracy: 98.00%\n\nValidation accuracy did not improve for 15 epoch(s).\n\n--- Epoch 94/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 94 | Loss: 0.0005 | Accuracy: 98.21%\nValidation set: Average loss: 0.0006 | Accuracy: 97.88%\n\nValidation accuracy did not improve for 16 epoch(s).\n\n--- Epoch 95/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 95 | Loss: 0.0004 | Accuracy: 98.21%\nValidation set: Average loss: 0.0005 | Accuracy: 98.02%\n\nValidation accuracy did not improve for 17 epoch(s).\n\n--- Epoch 96/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 96 | Loss: 0.0004 | Accuracy: 98.31%\nValidation set: Average loss: 0.0006 | Accuracy: 97.46%\n\nValidation accuracy did not improve for 18 epoch(s).\n\n--- Epoch 97/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 97 | Loss: 0.0004 | Accuracy: 98.23%\nValidation set: Average loss: 0.0006 | Accuracy: 97.74%\n\nValidation accuracy did not improve for 19 epoch(s).\n\n--- Epoch 98/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 98 | Loss: 0.0004 | Accuracy: 98.19%\nValidation set: Average loss: 0.0006 | Accuracy: 97.66%\n\nValidation accuracy did not improve for 20 epoch(s).\n\n--- Epoch 99/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 99 | Loss: 0.0005 | Accuracy: 98.21%\nValidation set: Average loss: 0.0006 | Accuracy: 97.88%\n\nValidation accuracy did not improve for 21 epoch(s).\n\n--- Epoch 100/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 100 | Loss: 0.0005 | Accuracy: 98.07%\nValidation set: Average loss: 0.0005 | Accuracy: 98.18%\n\nValidation accuracy did not improve for 22 epoch(s).\n\n--- Epoch 101/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 101 | Loss: 0.0005 | Accuracy: 98.08%\nValidation set: Average loss: 0.0006 | Accuracy: 97.86%\n\nValidation accuracy did not improve for 23 epoch(s).\n\n--- Epoch 102/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 102 | Loss: 0.0005 | Accuracy: 98.14%\nValidation set: Average loss: 0.0005 | Accuracy: 97.84%\n\nValidation accuracy did not improve for 24 epoch(s).\n\n--- Epoch 103/150 ---\nCurrent LR: 0.010000\nTrain Epoch: 103 | Loss: 0.0005 | Accuracy: 98.13%\nValidation set: Average loss: 0.0006 | Accuracy: 97.88%\n\nValidation accuracy did not improve for 25 epoch(s).\n\nEarly stopping triggered after 25 epochs without improvement.\n\nTraining finished.\n\nLoading best model from /kaggle/working/Partial_GCNN_best_tuned.pth for final evaluation...\nTest set no rotation: Average loss: 0.0004 | Accuracy: 98.39%\n\nTest set with rotation: Average loss: 0.0005 | Accuracy: 98.07%\n\n\nFinal Test Accuracy without test augmentation (Best Partial_GCNN Model): 98.39%\n\nFinal Test Accuracy with test augmentation (Best Partial_GCNN Model): 98.07%\nseed 4 \tP4-W (No Rotation)\t98.39%\nseed 4 \tP4-W (With Rotation)\t98.07%\nExpected Calibration Error no test rotation: 0.0039\nExpected Calibration Error with test rotation: 0.0038\nP4-W    : No Rotation - 98.39 Â± 0.00 (n=1)\nP4-W    : With Rotation - 98.07 Â± 0.00 (n=1)\n\n------- ECE Results ------------\nP4-W    : No Rotation - 0.003863693680614233\nP4-W    : With Rotation - 0.00375494547188282\n","output_type":"stream"}],"execution_count":6}]}